{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original shape: (737280, 33694)\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 92.5 GiB for an array with shape (737280, 33694) and data type float32",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mMemoryError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 14\u001b[39m\n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# Convert to dense format if necessary\u001b[39;00m\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(ad_sc.X, \u001b[33m'\u001b[39m\u001b[33mtoarray\u001b[39m\u001b[33m'\u001b[39m):\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m     ad_sc_dense = \u001b[43mad_sc\u001b[49m\u001b[43m.\u001b[49m\u001b[43mX\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtoarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     16\u001b[39m     ad_sc_dense = ad_sc.X\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\iqram\\.conda\\envs\\integration\\Lib\\site-packages\\scipy\\sparse\\_compressed.py:1170\u001b[39m, in \u001b[36m_cs_matrix.toarray\u001b[39m\u001b[34m(self, order, out)\u001b[39m\n\u001b[32m   1168\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m out \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m order \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1169\u001b[39m     order = \u001b[38;5;28mself\u001b[39m._swap(\u001b[33m'\u001b[39m\u001b[33mcf\u001b[39m\u001b[33m'\u001b[39m)[\u001b[32m0\u001b[39m]\n\u001b[32m-> \u001b[39m\u001b[32m1170\u001b[39m out = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_process_toarray_args\u001b[49m\u001b[43m(\u001b[49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1171\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (out.flags.c_contiguous \u001b[38;5;129;01mor\u001b[39;00m out.flags.f_contiguous):\n\u001b[32m   1172\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m'\u001b[39m\u001b[33mOutput array must be C or F contiguous\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\iqram\\.conda\\envs\\integration\\Lib\\site-packages\\scipy\\sparse\\_base.py:1367\u001b[39m, in \u001b[36m_spbase._process_toarray_args\u001b[39m\u001b[34m(self, order, out)\u001b[39m\n\u001b[32m   1365\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m out\n\u001b[32m   1366\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1367\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mzeros\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[43m=\u001b[49m\u001b[43morder\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mMemoryError\u001b[39m: Unable to allocate 92.5 GiB for an array with shape (737280, 33694) and data type float32"
     ]
    }
   ],
   "source": [
    "import scanpy as sc\n",
    "import numpy as np\n",
    "from sklearn.decomposition import IncrementalPCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load .h5ad file (scRNA)\n",
    "ad_sc = sc.read('../data/processed/processed_scrna_adata.h5ad')  # Use scanpy to read the file\n",
    "\n",
    "# Check the shape of the data\n",
    "print(f\"Original shape: {ad_sc.X.shape}\")\n",
    "\n",
    "# Convert to dense format if necessary\n",
    "if hasattr(ad_sc.X, 'toarray'):\n",
    "    ad_sc_dense = ad_sc.X.toarray()\n",
    "else:\n",
    "    ad_sc_dense = ad_sc.X\n",
    "\n",
    "# Ensure the data is in the correct numerical format\n",
    "ad_sc_dense = ad_sc_dense.astype(np.float32)\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "ad_sc_dense = scaler.fit_transform(ad_sc_dense)\n",
    "\n",
    "# Select a subset of samples (e.g., the first 2500 samples)\n",
    "subset_data = ad_sc_dense[:2500, :]  # Adjust this as needed\n",
    "\n",
    "# Initialize Incremental PCA to reduce features\n",
    "n_components = 50  # Specify the number of components you want to keep\n",
    "ipca = IncrementalPCA(n_components=n_components, batch_size=100)  # Use batch_size to control memory usage\n",
    "\n",
    "# Fit and transform the subset data in batches\n",
    "for i in range(0, subset_data.shape[0], 100):  # Adjust batch size as needed\n",
    "    ipca.partial_fit(subset_data[i:i + 100, :])\n",
    "\n",
    "# Transform the data\n",
    "reduced_data = ipca.transform(subset_data)\n",
    "\n",
    "# Print the shape of the reduced data\n",
    "print(f\"Reduced shape: {reduced_data.shape}\")\n",
    "\n",
    "# Create a new AnnData object to store the PCA results\n",
    "ad_sp_pca = sc.AnnData(X=reduced_data)\n",
    "\n",
    "# Optionally, you can add the original observation metadata\n",
    "# ad_sp_pca.obs = ad_sc.obs.iloc[:2500]  # If you used the first 2500 samples\n",
    "\n",
    "# Save the PCA results to a new .h5ad file\n",
    "ad_sp_pca.write('ad_sc_pca.h5ad')\n",
    "\n",
    "print(\"PCA results saved to 'ad_sc_pca.h5ad'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA on Single cell Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original shape: (737280, 33694)\n",
      "Using sparse matrix format...\n",
      "Available memory: 18.10 GB\n",
      "Reduced shape: (3500, 50)\n",
      "PCA results saved to 'ad_sc_pca.h5ad'\n"
     ]
    }
   ],
   "source": [
    "import scanpy as sc\n",
    "import numpy as np\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "import psutil\n",
    "\n",
    "# Load .h5ad file (scRNA)\n",
    "ad_sc = sc.read('../data/processed/processed_scrna_adata.h5ad')\n",
    "print(f\"Original shape: {ad_sc.X.shape}\")\n",
    "\n",
    "# Check if the matrix is sparse\n",
    "is_sparse = hasattr(ad_sc.X, 'toarray')\n",
    "\n",
    "if is_sparse:\n",
    "    print(\"Using sparse matrix format...\")\n",
    "\n",
    "# Select a subset of samples (first 2500 samples)\n",
    "subset_data = ad_sc.X[:3500, :]  # This will keep the data sparse if it's sparse\n",
    "\n",
    "# Memory check\n",
    "available_memory = psutil.virtual_memory().available / (1024 ** 3)\n",
    "print(f\"Available memory: {available_memory:.2f} GB\")\n",
    "\n",
    "# Initialize TruncatedSVD for sparse data\n",
    "n_components = 50\n",
    "svd = TruncatedSVD(n_components=n_components)\n",
    "\n",
    "# Fit and transform the subset data\n",
    "reduced_data = svd.fit_transform(subset_data)\n",
    "\n",
    "# Print the shape of the reduced data\n",
    "print(f\"Reduced shape: {reduced_data.shape}\")\n",
    "\n",
    "# Create a new AnnData object to store the PCA results\n",
    "ad_sc_pca = sc.AnnData(X=reduced_data)\n",
    "\n",
    "# Save the PCA results to a new .h5ad file\n",
    "ad_sc_pca.write('../data/processed/ad_sc_pca.h5ad')\n",
    "print(\"PCA results saved to 'ad_sc_pca.h5ad'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA on Visium spatial data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scanpy as sc\n",
    "import numpy as np\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "import psutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original shape: (3858, 18085)\n",
      "Using sparse matrix format...\n",
      "Available memory: 17.98 GB\n",
      "Reduced shape: (3500, 50)\n",
      "PCA results saved to 'ad_sp_pca.h5ad'\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Load .h5ad file (scRNA)\n",
    "ad_sp = sc.read('../data/processed/processed_visium_adata.h5ad')\n",
    "print(f\"Original shape: {ad_sp.X.shape}\")\n",
    "\n",
    "# Check if the matrix is sparse\n",
    "is_sparse = hasattr(ad_sp.X, 'toarray')\n",
    "\n",
    "if is_sparse:\n",
    "    print(\"Using sparse matrix format...\")\n",
    "\n",
    "# Select a subset of samples (first 2500 samples)\n",
    "subset_data = ad_sp.X[:3500, :]  # This will keep the data sparse if it's sparse\n",
    "\n",
    "# Memory check\n",
    "available_memory = psutil.virtual_memory().available / (1024 ** 3)\n",
    "print(f\"Available memory: {available_memory:.2f} GB\")\n",
    "\n",
    "# Initialize TruncatedSVD for sparse data\n",
    "n_components = 50\n",
    "svd = TruncatedSVD(n_components=n_components)\n",
    "\n",
    "# Fit and transform the subset data\n",
    "reduced_data = svd.fit_transform(subset_data)\n",
    "\n",
    "# Print the shape of the reduced data\n",
    "print(f\"Reduced shape: {reduced_data.shape}\")\n",
    "\n",
    "# Create a new AnnData object to store the PCA results\n",
    "ad_sp_pca = sc.AnnData(X=reduced_data)\n",
    "\n",
    "# Save the PCA results to a new .h5ad file\n",
    "ad_sp_pca.write('../data/processed/ad_sp_pca.h5ad')\n",
    "\n",
    "print(\"PCA results saved to 'ad_sp_pca.h5ad'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
